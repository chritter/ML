{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "#Spark context:\nsc", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7fb42899e400>"
                    }, 
                    "execution_count": 1, 
                    "metadata": {}
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "\nimport ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'loMFaVs2IMGcHX6VOhtUOTcoefFcd_jL7A2E7Dvn7HfF',\n    'service_id': 'iam-ServiceId-5283c20a-0811-43ec-9e27-271f0c96f146',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_cfd6857a4f194e3c98225364d85bad45_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initialized for you.\n# The following variable contains the path to your file on your IBM Cloud Object Storage.\npath_1 = cos.url('README.md', 'tests44c734db65ba4a57ba38d34b597e5326')\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "lines = sc.textFile(path_1)\nlines", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "cos://tests44c734db65ba4a57ba38d34b597e5326.os_cfd6857a4f194e3c98225364d85bad45_configs/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "lines_sp = lines.map(lambda x: x.split())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "lines_sp.take(2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[['#', 'Apache', 'Spark'], []]"
                    }, 
                    "execution_count": 5, 
                    "metadata": {}
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "# 2 partitions\nlines_sp.getNumPartitions()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "2"
                    }, 
                    "execution_count": 8, 
                    "metadata": {}
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "lines_sp2rep = lines_sp.repartition(4)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 10
        }, 
        {
            "source": "lines_sp2rep.getNumPartitions()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "4"
                    }, 
                    "execution_count": 11, 
                    "metadata": {}
                }
            ], 
            "execution_count": 11
        }, 
        {
            "source": "output = lines_sp2rep.collect()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "source": "lines_sp2rep_red = lines_sp2rep.filter(lambda x: len(x)>0)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 19
        }, 
        {
            "source": "lines_sp2rep_red.take(2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[['##', 'Online', 'Documentation'],\n ['You',\n  'can',\n  'find',\n  'the',\n  'latest',\n  'Spark',\n  'documentation,',\n  'including',\n  'a',\n  'programming']]"
                    }, 
                    "execution_count": 20, 
                    "metadata": {}
                }
            ], 
            "execution_count": 20
        }, 
        {
            "source": "### Mapping functions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\nrdd.map(lambda x: x+(x[1],x[0])).collect()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "lines = sc.parallelize([\"hello world\", \"hi\"]) \nwords = lines.flatMap(lambda line: line.split(\" \"))\nwords.collect()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "['hello', 'world', 'hi']"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),(\"b\",[\"p\", \"r\"])]) \nrdd4.flatMapValues(lambda x: x+['add me to value list']).collect()    ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[('a', 'x'),\n ('a', 'y'),\n ('a', 'z'),\n ('a', 'add me to value list'),\n ('b', 'p'),\n ('b', 'r'),\n ('b', 'add me to value list')]"
                    }, 
                    "execution_count": 4, 
                    "metadata": {}
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "import numpy as np", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "np.array(['start']).append(np.random.normal(size=5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "unexpected EOF while parsing (<ipython-input-6-c3c4ed7c5f0a>, line 1)", 
                    "traceback": [
                        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-c3c4ed7c5f0a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    np.array(['start']).append(np.random.normal(size=5)\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
                    ], 
                    "ename": "SyntaxError"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "rdd5 = rdd4.mapValues(lambda x: np.array(['start']).append(np.random.normal(size=5)))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "rdd5.collect()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#instead of distinct\nrdd5.mapValues(lambda x: xnp.unique(x)).collect()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "rdd4.mapValues?", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def f(x): print(x,'a')\na=sc.parallelize([1, 2, 3, 4, 5])\na.foreach(f)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 18
        }, 
        {
            "source": "a.collect()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[1, 2, 3, 4, 5]"
                    }, 
                    "execution_count": 20, 
                    "metadata": {}
                }
            ], 
            "execution_count": 20
        }, 
        {
            "source": "rdd4.countByKey()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "defaultdict(int, {'a': 1, 'b': 1})"
                    }, 
                    "execution_count": 21, 
                    "metadata": {}
                }
            ], 
            "execution_count": 21
        }, 
        {
            "source": "#rdd4.countByValue()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 23
        }, 
        {
            "source": "rdd.describe().show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "error", 
                    "evalue": "'RDD' object has no attribute 'describe'", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-24-7b41116f0d2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[1;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'describe'"
                    ], 
                    "ename": "AttributeError"
                }
            ], 
            "execution_count": 24
        }, 
        {
            "source": "### Spark dataframes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_spark.describe().show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.2", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}